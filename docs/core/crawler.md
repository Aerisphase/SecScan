# Модуль Crawler

## Обзор
Модуль `crawler.py` реализует функциональность обхода веб-сайтов для сбора информации о страницах, формах и ссылках. Использует асинхронное программирование для эффективного параллельного сканирования.

## Основные компоненты

### Класс AdvancedCrawler
Основной класс, реализующий функциональность обхода сайтов.

#### Методы:
- `__init__(self, base_url: str, max_pages: int = 20, delay: float = 0.1, user_agent: Optional[str] = None)`
  - Инициализация краулера
  - Параметры:
    - `base_url`: Базовый URL для сканирования
    - `max_pages`: Максимальное количество страниц для сканирования
    - `delay`: Задержка между запросами
    - `user_agent`: Пользовательский User-Agent

- `async crawl(self) -> List[Dict]`
  - Основной метод обхода сайта
  - Возвращает список обнаруженных страниц с их данными

- `async _worker(self)`
  - Рабочий процесс для параллельного сканирования
  - Обрабатывает URL из очереди

- `async _crawl_page(self, url: str)`
  - Обработка отдельной страницы
  - Параметры:
    - `url`: URL страницы для обработки

- `_extract_forms(self, soup: BeautifulSoup) -> List[Dict]`
  - Извлечение данных форм со страницы
  - Параметры:
    - `soup`: Объект BeautifulSoup с разобранной страницей
  - Возвращает список форм с их атрибутами

- `_extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]`
  - Извлечение и нормализация ссылок со страницы
  - Параметры:
    - `soup`: Объект BeautifulSoup с разобранной страницей
    - `base_url`: Базовый URL для нормализации относительных ссылок
  - Возвращает список абсолютных URL

### Вспомогательные функции

#### `scan_website(target_url: str, config: Dict) -> Optional[Dict]`
- Обертка для сканирования сайта
- Параметры:
  - `target_url`: URL для сканирования
  - `config`: Конфигурация сканирования
- Возвращает результаты сканирования

## Формат данных

### Данные страницы
```python
{
    'url': str,
    'title': str,
    'forms': [
        {
            'action': str,
            'method': str,
            'inputs': [
                {
                    'name': str,
                    'type': str,
                    'value': str
                }
            ]
        }
    ],
    'links': [str],
    'content': str
}
```

### Форма
```python
{
    'action': str,
    'method': str,
    'inputs': [
        {
            'name': str,
            'type': str,
            'value': str
        }
    ]
}
```

## Пример использования

```python
from core.crawler import AdvancedCrawler
import asyncio

async def main():
    crawler = AdvancedCrawler(
        base_url='https://example.com',
        max_pages=50,
        delay=0.5
    )
    
    async with crawler:
        pages = await crawler.crawl()
        print(f"Найдено {len(pages)} страниц")
        
        for page in pages:
            print(f"URL: {page['url']}")
            print(f"Форм: {len(page['forms'])}")
            print(f"Ссылок: {len(page['links'])}")

asyncio.run(main())
```

## Особенности реализации

1. **Асинхронное программирование**
   - Использование `asyncio` для параллельного сканирования
   - Ограничение количества одновременных запросов
   - Очередь URL для обработки

2. **Управление соединениями**
   - Пул соединений с ограничением
   - Кэширование DNS
   - Таймауты запросов

3. **Обработка ошибок**
   - Логирование ошибок
   - Продолжение работы при ошибках отдельных запросов
   - Корректное закрытие соединений

## Рекомендации по использованию

1. Настройте параметры в зависимости от размера сайта:
   - `max_pages`: Увеличьте для больших сайтов
   - `delay`: Уменьшите для быстрого сканирования
   - `semaphore`: Настройте количество одновременных запросов

2. Используйте контекстный менеджер (`async with`) для корректного закрытия соединений

3. Обрабатывайте исключения при сканировании больших сайтов

4. Настройте User-Agent для имитации реального браузера 